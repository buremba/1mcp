# AI Model Configuration
# Copy this file to .env.local and customize for your setup

# AI Provider Selection
# Options: chrome (default), openai, anthropic, groq, ollama
VITE_AI_PROVIDER=chrome

# Chrome Prompt API (local, no API key needed)
# This is the default provider, requires Chrome 129+ with flags enabled

# OpenAI Configuration
# Get your API key from: https://platform.openai.com/api-keys
# VITE_OPENAI_API_KEY=sk-...
# VITE_OPENAI_MODEL=gpt-4o
# Other options: gpt-4o-mini (faster/cheaper), o1-preview (reasoning), o1-mini

# Anthropic Claude Configuration
# Get your API key from: https://console.anthropic.com/
# VITE_ANTHROPIC_API_KEY=sk-ant-...
# VITE_ANTHROPIC_MODEL=claude-3-5-sonnet-20241022

# Groq Configuration (fast inference)
# Get your API key from: https://console.groq.com/
# VITE_GROQ_API_KEY=gsk_...
# VITE_GROQ_MODEL=llama-3.1-70b-versatile

# Ollama Configuration (local models)
# Ensure Ollama is running locally: ollama serve
# VITE_OLLAMA_BASE_URL=http://localhost:11434
# VITE_OLLAMA_MODEL=llama3:latest

# Relay-MCP Server Configuration
VITE_RELAY_SERVER_URL=http://127.0.0.1:7888

# Feature Flags
# VITE_ENABLE_MARKDOWN=true
# VITE_ENABLE_MERMAID=true
# VITE_ENABLE_SYNTAX_HIGHLIGHTING=true
